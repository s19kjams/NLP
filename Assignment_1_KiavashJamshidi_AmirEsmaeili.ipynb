{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def extract_words_tokens(any_string: str):\n",
    "    without_space_string = any_string.strip().split()\n",
    "    num_words = len(without_space_string)\n",
    "    num_tokens = len(any_string)\n",
    "    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def lemmatize(any_string, file_name):\n",
    "    lemma_dict = {}\n",
    "    with open(file_name, 'r', encoding=\"utf-8-sig\") as file:\n",
    "        for line in file:\n",
    "            lemma, word_form = line.strip().split(\"\\t\")\n",
    "            lemma_dict[word_form] = lemma\n",
    "    \n",
    "    input_words = any_string.split()\n",
    "    \n",
    "    dictionary_of_lemmatized_words = {}\n",
    "    for word in input_words:\n",
    "        word_without_special_characters = ''.join(e for e in word if e.isalnum())\n",
    "        lower_word_without_special_characters = word_without_special_characters.lower()\n",
    "\n",
    "        dictionary_of_lemmatized_words[word_without_special_characters] = lemma_dict.get(lower_word_without_special_characters, lower_word_without_special_characters)\n",
    "    \n",
    "    return dictionary_of_lemmatized_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'output.csv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "\n",
    "def create_count_and_probability(file_name):\n",
    "    with open(file_name, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "    \n",
    "    vocabulary = set()\n",
    "    for line in lines:\n",
    "        lemmatized_words = lemmatize(line, \"lemmatization-en.txt\").values()\n",
    "        vocabulary.update(lemmatized_words)\n",
    "\n",
    "    vocabulary = sorted(vocabulary)\n",
    "    \n",
    "    with open('output.csv', 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow(['Text', 'Count_Vector', 'Probability']) \n",
    "        \n",
    "        for line in lines:\n",
    "            lemmatized_words_dict = lemmatize(line, \"lemmatization-en.txt\")\n",
    "\n",
    "            lemmatized_words = []\n",
    "            for word in line.split():\n",
    "                word_without_special_characters = ''.join(e for e in word if e.isalnum()).lower()\n",
    "                lemmatized_word = lemmatized_words_dict.get(word_without_special_characters, word_without_special_characters)\n",
    "                lemmatized_words.append(lemmatized_word)\n",
    "            \n",
    "            word_count = Counter(lemmatized_words)\n",
    "            \n",
    "            count_vector = [word_count[word] for word in vocabulary]\n",
    "\n",
    "            total_words = sum(count_vector)\n",
    "            probability_vector = [word_count[word] / total_words if total_words > 0 else 0 for word in lemmatized_words]\n",
    "            writer.writerow([line.strip(), count_vector, probability_vector])\n",
    "\n",
    "    return ('output.csv')\n",
    "\n",
    "create_count_and_probability(\"corpus.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
