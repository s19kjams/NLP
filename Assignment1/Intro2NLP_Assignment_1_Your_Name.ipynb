{"cells":[{"cell_type":"markdown","id":"5a325897","metadata":{"id":"5a325897"},"source":["# Introduction to Natural Language Processing: Assignment 1\n","\n","In this assignment we'll practice word operations and text classifications.\n","\n","- Please comment your code\n","- You can use built-in Python packages, scikit-learn and Pandas.\n","- Submissions are due **on Tuesdays at 23:59** and should be submitted **ONLY** on eCampus: **Assignmnets >> Student Submissions >> Assignment 1 (Deadline: 05.11.2024, at 23:59)**\n","- Name the file aproppriately \"Assignment_1_\\<Kiavash>.ipynb\".\n","- Please submit **ONLY** the Jupyter Notebook file.\n","- Please use relative path; Your code should work on my computer if the Jupyter Notebook and the file are both in the same directory.\n","\n","Example: file_name = lemmatization-en.txt >> **DON'T use:** /Users/ComputerName/Username/Documents/.../lemmatization-en.txt"]},{"cell_type":"markdown","id":"0cd8bf33","metadata":{"id":"0cd8bf33"},"source":["### Task 1.1 (2 points)\n","\n","Write a function `extract_words_tokens(any_string)` that takes a string as input and returns two numbers:\n","1. num_words: The number of words in string\n","2. num_tokens: The number of tokens in string (Please use the character-based tokenization.)\n","\n","**Hint:** The string can be a single word or a sentence and\n"," can contain some special charecters, such as: \"!\", \",\", \":\""]},{"cell_type":"code","execution_count":null,"id":"f14f3124","metadata":{"id":"f14f3124"},"outputs":[],"source":["def extract_words_tokens(any_string):\n","    #here comes your code\n","    return(print(any_string, \":\", \"num_words:\", num_words, \"and\", \"num_tokens:\", num_tokens, \"respectively\"))"]},{"cell_type":"markdown","id":"a4b05add","metadata":{"id":"a4b05add"},"source":["### Task 1.2 (4 points)\n","\n","Write a function `lemmatize(any_string, file_name)` that takes as input any string and a file-name: `lemmatization-en.txt` (please download the file [here](https://github.com/michmech/lemmatization-lists/blob/master/lemmatization-en.txt). It's a tab separated corpus) and returns a dictionary with all words as keys and the lemma of the words as values.\n","\n","**Hint:** To tokenize the string, please use the whitespace as the seperator. The string doesn't contain any special characters."]},{"cell_type":"code","execution_count":null,"id":"a12f48ff","metadata":{"id":"a12f48ff"},"outputs":[],"source":["def lemmatize(any_string, file_name):\n","    #here comes your code\n","    return(print(dictionary_of_lemmatized_words))"]},{"cell_type":"markdown","id":"c5142598","metadata":{},"source":["### Task 2 (1 point)\n","\n","Create a DataFrame using the `polarity.txt` file and give name to the columns appropriately. (e.g., \"Text\", \"Label\")"]},{"cell_type":"code","execution_count":null,"id":"e400715a","metadata":{},"outputs":[],"source":["#here comes your code"]},{"cell_type":"markdown","id":"23b5eaf4","metadata":{},"source":["### Task 2.1 (2 point)\n","\n","Create a new column for the DataFrame that contains labels converted to numerical values instead of strings using the function: `apply()` and drop the original column afterwards.\n","\n","Hint: The numarical values can be any meaningful values, e.g., pos >> 1 and neg >> 0"]},{"cell_type":"code","execution_count":null,"id":"8309aebd","metadata":{},"outputs":[],"source":["# here comes your code"]},{"cell_type":"markdown","id":"58b7e88b","metadata":{},"source":["### Task 3 (7 points)\n","\n","Write a function `create_count_and_probability` that takes a file (`corpus.txt`) as input and returns a csv file as output containing three columns:\n","1. Text\n","2. Count_Vector\n","3. Probability\n","\n","Example:\n","\n","For the line: `This document is the second document.`\n","\n","The row in the csv file should contain:\n","`This document is the second document.`   `[0,2,0,1,0,1,1,0,1]`   `[1/6, 2/6, 1/6, 1/6, 1/6, 2/6]`\n","\n","**Note**:\n","\n","1. You should define your own function and not use e.g., CountVectorizer() which gives you the `count vector`, directly.\n","\n","2. You can either use the whitespace in `split` as the seperator or use the `Regular Expression (re)` to extract the words, as follows:\n","\n","```\n","import re\n","TEXT = \"Hey, - How are you doing today!?\"\n","words_list = re.findall(r\"[\\w']+\", TEXT)\n","print(words_list)\n","```\n","\n","3. To count the words, you can use e.g., the library: `collections`, more specifically `Counter`.\n","\n","4. Please don't upload the output file. Your function should generate the file."]},{"cell_type":"code","execution_count":null,"id":"26f9f9be","metadata":{},"outputs":[],"source":["def create_count_and_probability(file_name):\n","    # here comes your code\n","    return(csv_file)"]},{"cell_type":"markdown","id":"e701a2ce","metadata":{},"source":["### Task 4 (8 points)\n","\n","The goal of this task is to train and test classifiers provided in scikit-learn, using two datasets `rural.txt` and `science.txt`.\n","\n","a) Each file (rural and science) contains sentence-wise documents. You should create a dataframe containing two columns: \"Document\" and \" Class\", as shown below. This dataframe will be used later as input for the vectorizer.\n","\n","|Document                             |Class |\n","| ------------------------------------|----- |\n","|PM denies knowledge of AWB kickbacks | rural |\n","|The crocodile ancestor fossil, found...| science |\n","\n","\n","b) Remove stop words from the data and create two separate plots showing word frequency for documents in each label.\n","\n","c) Split the data into train (70%) and test (30%) sets and use the following vectorization techniquess to train the two classifiers provided by scikit-learn:\n","\n","- one-hot-encoding\n","- count vectorization\n","\n","Classifiers:\n","- naive_bayes.GaussianNB()\n","- LogisticRegression()\n","\n","**Hints:**\n","1. The Gaussian NB Classifier takes a dense matrix as input and the output of the vectorizer is a sparse matrix. Use my_matrix.toarray() for this conversion.\n","2. You can play around with various parameters in both the count-vectorizer and the classifier to get a better performance in terms of the accuracy. (In the exercise, we will discuss the accuracy of your model.)"]},{"cell_type":"code","execution_count":null,"id":"c19367cf","metadata":{},"outputs":[],"source":["# Here comes your code"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":5}
