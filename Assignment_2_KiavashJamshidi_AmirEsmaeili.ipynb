{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Amir Esmaeili, 50225893, s77aesma@uni-bonn.de\n",
    "* Kiavash Jamshidi, 50151853, s19kjams@uni-bonn.de"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 (2 points)\n",
    "This function, extract_proper_nouns, takes a file name as input, reads its content,\n",
    "and returns a list of multi-word proper nouns found in the text.\n",
    "It uses scikit-learn's CountVectorizer to split the text into tokens (words).\n",
    "The function then identifies sequences of words that start with an uppercase letter,\n",
    "considering them as proper nouns if they contain more than one consecutive word.\n",
    "Finally, it returns a list of these multi-word proper nouns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def extract_proper_nouns(my_file_name):\n",
    "    several_token_propn = []\n",
    "    \n",
    "    with open(my_file_name, 'r') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    vectorizer = CountVectorizer().build_tokenizer()\n",
    "    tokens = vectorizer(text)\n",
    "    \n",
    "    current_propn = []\n",
    "    for word in tokens:\n",
    "        if word[0].isupper():\n",
    "            current_propn.append(word)\n",
    "        else:\n",
    "            if len(current_propn) > 1:\n",
    "                several_token_propn.append(\" \".join(current_propn))\n",
    "            current_propn = []\n",
    "    \n",
    "    if len(current_propn) > 1:\n",
    "        several_token_propn.append(\" \".join(current_propn))\n",
    "    \n",
    "    return several_token_propn\n",
    "\n",
    "extract_proper_nouns(\"test.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Lemma Extraction Function\n",
    "\n",
    "This notebook defines a function `common_lemma(my_file_name)` that processes a text file and returns a dictionary where:\n",
    "- **Key**: The lemma (root form) of the word.\n",
    "- **Value**: A list of words that share the same lemma and are either nouns or verbs.\n",
    "\n",
    "### Function Workflow:\n",
    "1. The function reads the input file and processes its content using the **spaCy** library.\n",
    "2. It tokenizes the text and checks if each token is either a **noun** or a **verb**.\n",
    "3. For each noun/verb, the function retrieves the lemma and groups the words that share the same lemma.\n",
    "4. It only includes lemmas that appear both as nouns and verbs in the text.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'show': ['show', 'showing', 'showed']}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def common_lemma(my_file_name):\n",
    "    tokens_with_common_lemma = {}\n",
    "\n",
    "    with open(my_file_name, 'r') as file:\n",
    "        text = file.read()\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.pos_ in {\"NOUN\", \"VERB\"}:\n",
    "            lemma = token.lemma_\n",
    "            \n",
    "            if lemma not in tokens_with_common_lemma:\n",
    "                tokens_with_common_lemma[lemma] = []\n",
    "            \n",
    "            if token.text not in tokens_with_common_lemma[lemma]:\n",
    "                tokens_with_common_lemma[lemma].append(token.text)\n",
    "    \n",
    "    tokens_with_common_lemma = {lemma: words for lemma, words in tokens_with_common_lemma.items() \n",
    "                                if len(words) > 1}\n",
    "    \n",
    "    return tokens_with_common_lemma\n",
    "common_lemma(\"test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BBC News Dataset Loading\n",
    "\n",
    "This notebook demonstrates how to load and inspect the **BBC News dataset** using the **pandas** library.\n",
    "\n",
    "### Steps:\n",
    "1. The function loads the **`bbc-news.csv`** file into a **pandas DataFrame**.\n",
    "2. The first few rows of the dataset are displayed using `head()`, allowing us to preview the structure and content of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        category                                               text\n",
      "0           tech  tv future in the hands of viewers with home th...\n",
      "1       business  worldcom boss  left books alone  former worldc...\n",
      "2          sport  tigers wary of farrell  gamble  leicester say ...\n",
      "3          sport  yeading face newcastle in fa cup premiership s...\n",
      "4  entertainment  ocean s twelve raids box office ocean s twelve...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'bbc-news.csv' \n",
    "bbc_data = pd.read_csv(file_path)\n",
    "\n",
    "print(bbc_data.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
