{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1 (2 points)\n",
    "This function, extract_proper_nouns, takes a file name as input, reads its content,\n",
    "and returns a list of multi-word proper nouns found in the text.\n",
    "It uses scikit-learn's CountVectorizer to split the text into tokens (words).\n",
    "The function then identifies sequences of words that start with an uppercase letter,\n",
    "considering them as proper nouns if they contain more than one consecutive word.\n",
    "Finally, it returns a list of these multi-word proper nouns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hong Kong', 'New York']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def extract_proper_nouns(my_file_name):\n",
    "    several_token_propn = []\n",
    "    \n",
    "    with open(my_file_name, 'r') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    vectorizer = CountVectorizer().build_tokenizer()\n",
    "    tokens = vectorizer(text)\n",
    "    \n",
    "    current_propn = []\n",
    "    for word in tokens:\n",
    "        if word[0].isupper():\n",
    "            current_propn.append(word)\n",
    "        else:\n",
    "            if len(current_propn) > 1:\n",
    "                several_token_propn.append(\" \".join(current_propn))\n",
    "            current_propn = []\n",
    "    \n",
    "    if len(current_propn) > 1:\n",
    "        several_token_propn.append(\" \".join(current_propn))\n",
    "    \n",
    "    return several_token_propn\n",
    "\n",
    "extract_proper_nouns(\"test.txt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'show': ['showing', 'showed']}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def common_lemma(my_file_name):\n",
    "    tokens_with_common_lemma = {}\n",
    "\n",
    "    with open(my_file_name, 'r') as file:\n",
    "        text = file.read()\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.pos_ in {\"NOUN\", \"VERB\"}:\n",
    "            lemma = token.lemma_\n",
    "            \n",
    "            if lemma not in tokens_with_common_lemma:\n",
    "                tokens_with_common_lemma[lemma] = []\n",
    "            \n",
    "            if token.text not in tokens_with_common_lemma[lemma]:\n",
    "                tokens_with_common_lemma[lemma].append(token.text)\n",
    "    \n",
    "    tokens_with_common_lemma = {lemma: words for lemma, words in tokens_with_common_lemma.items() \n",
    "                                if len(words) > 1}\n",
    "    \n",
    "    return tokens_with_common_lemma\n",
    "common_lemma(\"test.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
