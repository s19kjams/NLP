{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c339230a-ed92-45af-a07b-eea11572b1d8",
   "metadata": {
    "id": "c339230a-ed92-45af-a07b-eea11572b1d8"
   },
   "source": [
    "# Introduction to Natural Language Processing: Assignment 3\n",
    "\n",
    "In this exercise we'll practice training RNN & LSTM models as well as fine-tuning LLMs to predict one or more labels for a given text using Hugging Face and PyTorch.\n",
    "\n",
    "- You can use any Python package you need.\n",
    "- Please comment your code\n",
    "- Submissions are due Tuesdays at 23:59 **only** on eCampus: **Assignmnets >> Student Submissions >> Assignment 3 (Deadline: 10.12.2024, at 23:59)**\n",
    "\n",
    "- Name the file aproppriately: \"Assignment_3_\\<Your_Name\\>.ipynb\" and submit only the Jupyter Notebook file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c845daa-4566-4abe-9914-2f9b35544c31",
   "metadata": {},
   "source": [
    "### Task 1 (15 points)\n",
    "\n",
    "In this task you will implement text generation in torch with a multi-layer RNN and multi-layer LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b41569-0edf-4653-a059-17e54b510fb2",
   "metadata": {},
   "source": [
    "a) Implement the missing methods of the dataset class to\n",
    "1. load the dataset from the file `reddit-cleanjokes.csv` and split it into words [**(dataset link)**](https://raw.githubusercontent.com/amoudgl/short-jokes-dataset/master/data/reddit-cleanjokes.csv)\n",
    "2. get a list of the unique words\n",
    "3. implement the `__getitem__` method to iterate through the dataset. Hint: use `torch.tensor` to turn a list into a tensor.\n",
    "\n",
    "Then instantiate the dataset with `sequence_length=4`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80f6dac7-61cb-4373-b6a5-37e12856c78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import List\n",
    "# hint: use these methods\n",
    "from pandas import read_csv\n",
    "from collections import Counter\n",
    "\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        sequence_length,\n",
    "    ):\n",
    "        self.sequence_length = sequence_length\n",
    "        self.words = self.load_words()\n",
    "        self.uniq_words = self.get_uniq_words()\n",
    "\n",
    "        self.index_to_word = {index: word for index, word in enumerate(self.uniq_words)}\n",
    "        self.word_to_index = {word: index for index, word in enumerate(self.uniq_words)}\n",
    "\n",
    "        self.words_indexes = [self.word_to_index[w] for w in self.words]\n",
    "\n",
    "    def load_words(self) -> List[str]:\n",
    "        \"\"\"Returns a list of all words in the dataset.\n",
    "        Make sure to strip punctuation and lowercase the words\"\"\"\n",
    "       # YOUR CODE HERE\n",
    "\n",
    "    def get_uniq_words(self) -> List[str]:\n",
    "        \"\"\"Returns a list, containing each unique word in the dataset once\"\"\"\n",
    "       # YOUR CODE HERE\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Returns the number of `self.sequence_length` length word spans in the dataset\"\"\"\n",
    "        return len(self.words_indexes) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, index) -> (Tensor, Tensor):\n",
    "        \"\"\"Returns a tuple of two torch.Tensors:\n",
    "        an input sequence for the RNN/LSTM model and a target sequence.\n",
    "        The tensors should be 1D and have length equal to self.sequence_length.\n",
    "        Remember that the target should be shifted with respect to the input.\"\"\"\n",
    "       # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67e2030-39ab-4e7d-874e-3bcb1ad35272",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset(4)\n",
    "dataset.load_words()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d8815e-56b9-451f-b4ee-e1ec7273145c",
   "metadata": {},
   "source": [
    "b) Now, complete the implementation of the RNN model.\n",
    "\n",
    "You'll need to use all the model components defined in `__init__`: [Embedding](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html), [RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html), and the [Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) output layer. These are all subclasses of [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html).\n",
    "\n",
    "Torch Modules are objects that hold the layer's weights and biases (called parameters, accessed by `model.parameters()`) and keep track of a bunch of metadata, like what device the weights are on or what precision they're stored at. Each Module can have parts that are themselves Modules. The easiest way to combine Modules is with [`torch.nn.Sequential`](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html). \n",
    "\n",
    "Every Module must have a `forward` method. This defines what the Module does with its input and returns as output. You can access `forward` simply by calling the Module, for example `output = self.rnn(input)`. This is the preferred way to write it.\n",
    "\n",
    "Hint: unlike the one we saw in the tutorial, this RNN has multiple layers. Think about what this means for the shape of the hidden state. You might not want to use Sequential as RNN has multiple inputs and outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6c0cf6-d662-4dcb-a8dc-298d2c92d347",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, dataset):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_size = 128\n",
    "        self.embedding_dim = 128\n",
    "        self.num_layers = 3\n",
    "\n",
    "        n_vocab = len(dataset.uniq_words)\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings=n_vocab,\n",
    "            embedding_dim=self.embedding_dim,\n",
    "        )\n",
    "        self.rnn = nn.RNN(\n",
    "            input_size=self.hidden_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.num_layers,\n",
    "            dropout=0.2,\n",
    "        )\n",
    "        self.fc = nn.Linear(self.hidden_size, n_vocab)\n",
    "\n",
    "    def init_state(self, sequence_length: int) -> Tensor:\n",
    "        \"\"\"Returns the initial state hidden state (all zeros), with the correct shape.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "    \n",
    "    def forward(self, inputs: Tensor, prev_hidden_state: Tensor) -> (Tensor, Tensor):\n",
    "        \"\"\"Compute the logits and next_hidden_state.\"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        return logits, next_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d1b66e0-16db-494f-ae56-bb252f2ae218",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNNModel(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c58e452-8530-40d4-8208-694525c7c161",
   "metadata": {},
   "source": [
    "c) Write a function that counts the total number of parameters and total number of trainable parameters of a model.\n",
    "\n",
    "Refer to the [torch documentation](https://pytorch.org/docs/stable/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263e22b4-570c-47d9-b2ab-74e73e337d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(model: nn.Module) -> (int, int):\n",
    "    # YOUR CODE HERE\n",
    "    return n_params, n_trainable_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b985f9-f444-4e5b-83e7-ab205838d669",
   "metadata": {},
   "source": [
    "d) Complete the training loop and train the model for 10 epochs. Store the training loss in a list. You will probably want to have an inner loop that loops over batches.\n",
    "\n",
    "Hint: refer to the documentation of the `DataLoader`, `CrossEntropyLoss` and `Optimizer` classes. You might also need to use the `detach()` and `item()` methods to work with the loss tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5529bd12-a84b-493a-819a-191beee539a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_rnn(dataset, model, sequence_length, batch_size, max_epochs) -> List[float]:\n",
    "    model.train()\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_list = []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        # YOUR CODE HERE\n",
    "\n",
    "    return loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6784e538-36ae-4b06-b285-ee48459cb49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_rnn = train_rnn(dataset, model, 4, 256, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f533ddb-6199-4ec4-9fe8-3433191d94a5",
   "metadata": {},
   "source": [
    "e) Complete the function to generate output from the model using 1. argmax (greedy) decoding 2. softmax decoding. Generate some sample outputs with each and discuss briefly.\n",
    "\n",
    "Hint: torch already has a builtin function for getting the softmax of a tensor, which you may use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c826b3-6073-4425-be68-bffc6d43c5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_rnn_argmax(dataset: Dataset, model: nn.Module, text: str, next_words=100) -> List[str]:\n",
    "    model.eval()\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return words\n",
    "\n",
    "def predict_rnn_softmax(dataset: Dataset, model: nn.Module, text: str, next_words=100) -> List[str]:\n",
    "    model.eval()\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b1ea38-6a51-428c-8c0c-02e12a616ce5",
   "metadata": {},
   "source": [
    "f) Implement `LSTMModel`, `train_lstm`, `predict_lstm_argmax` and `predict_lstm_softmax`. Train the model using the same settings and plot both training loss curves together. Briefly discuss the differences in the model architectures and performance. Which model performs better and what are possible causes? What are the limitations of the model?\n",
    "\n",
    "Hint: use the `torch.nn.LSTM` class. You can do almost everything as with RNN, but take into account that an LSTM has **two** hidden states.\n",
    "\n",
    "Hint: You might not necessarily see that LSTM performs better even if your implementation is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9237de0-a2b6-4bc7-b670-6b2767a19c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here comes your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54db2146",
   "metadata": {
    "id": "54db2146"
   },
   "source": [
    "### Task 2 (2 points)\n",
    "\n",
    "The goal of this task is to download a multi-label text classification dataset from the [Hugging Face Hub](https://huggingface.co/datasets) and load it.\n",
    "\n",
    "a) Select the `Text Classification` tag on the left, multi-label-classificationas as well as the the \"1K<n<10K\" tag to find a relatively small dataset. (e.g., sem_eval_2018_task_1 >> subtask5.english)\n",
    "\n",
    "b) Load your dataset using `load_dataset` and check (print) the last data point in the validation set.\n",
    "\n",
    "**Hint:** If you don't have access to GPU, you can downsample the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8287d740",
   "metadata": {
    "id": "8287d740"
   },
   "outputs": [],
   "source": [
    "# Here comes your code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f787dbb",
   "metadata": {
    "id": "3f787dbb"
   },
   "source": [
    "### Task 3 (3 points)\n",
    "\n",
    "a) Write a function `tokenize_data(dataset)` that takes the loaded dataset as input and returns the encoded dataset for both text and labels.\n",
    "\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "1. You should tokenize the text using the BERT tokenizer `bert-base-uncased`\n",
    "2. You also need to provide labels to the model as numbers. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). This should be a tensor of floats rather than integers.\n",
    "3. You can apply the function `tokenize_data(dataset)` to the the dataset using `map()`. (You can check out the exercise!)\n",
    "4. You should set the format of the data to PyTorch tensors using `encoded_dataset.set_format(\"torch\")`. This will turn the training, validation and test sets into standard PyTorch.\n",
    "\n",
    "b) Print the `keys()` of the the last data point in the validation set in the encoded dataset.\n",
    "\n",
    "**Hint:** The output should be as follows:\n",
    "\n",
    "`dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'labels'])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7431dc",
   "metadata": {
    "id": "0f7431dc"
   },
   "outputs": [],
   "source": [
    "def tokenize_data(dataset):\n",
    "    # here comes your code\n",
    "    return(encoded_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hafPL0Yh6S8W",
   "metadata": {
    "id": "hafPL0Yh6S8W"
   },
   "source": [
    "### Task 4 (15 points)\n",
    "\n",
    "Implement and compare two different approaches for text classification on a multilabel dataset. One approach will utilize BERT for tokenization and classification, while the other will use an alternative method such as (TF-IDF + SVM) or (BOW + LR).\n",
    "\n",
    "a) **BERT Approach:**\n",
    "\n",
    "\n",
    "``1.`` Define a text classification model that includes a pre-trained base ``(bert-base-uncased)`` using ``AutoModelForSequenceClassification``.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "       \n",
    "- Create two dictionaries that map labels to integers and vice versa for the ``id2label`` and ``label2id`` parameters in  `.from_pretrained function` .\n",
    "        \n",
    "- Set the `problem_type` to \"multi_label_classification\" to ensure the appropriate loss function is used.\n",
    "        \n",
    "``2.`` Train the BERT-based model using HuggingFace's Trainer API.\n",
    "\n",
    "**Hints:**\n",
    "- Utilize `TrainingArguments` and `Trainer` classes.\n",
    "\n",
    "- While training, compute metrics using a ``compute_metrics`` function that returns a dictionary with the desired metric values.\n",
    "\n",
    "b) **Alternative Approach:**\n",
    "\n",
    "\n",
    "``1.`` Choose an alternative approach for tokenization and classification. For example, use TF-IDF  or Bag of Words (BoW) for tokenization and a traditional classifier like SVM or logistic regression for classification.\n",
    "\n",
    "**Hints:**\n",
    "\n",
    "  - Use scikit-learn library for the  implementations.\n",
    "\n",
    "``2.`` Train the alternative approach (model) on the same dataset you used for the BERT approach.\n",
    "\n",
    "__Hints:__\n",
    "\n",
    "  - Use appropriate training and evaluation procedures for the chosen alternative approach.\n",
    "  \n",
    "``3.`` Evaluate the performance of both models on the validation set using the metrics Accuracy, F1-score, precision, recall.\n",
    "\n",
    "c) **Discussion:**\n",
    "\n",
    " Discuss the strengths and weaknesses of each approach.\n",
    "\n",
    "__Note:__ Feel free to explore variations and improvements for both approaches. Experiment with hyperparameters and preprocessing steps to enhance the models' performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceV8_EJ6S8W",
   "metadata": {
    "id": "cceV8_EJ6S8W"
   },
   "outputs": [],
   "source": [
    "# Here comes your code for BERT Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IhqwKwMe6S8X",
   "metadata": {
    "id": "IhqwKwMe6S8X"
   },
   "outputs": [],
   "source": [
    "# Here comes your code for the alternative approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2yaPfg6n6S8X",
   "metadata": {
    "id": "2yaPfg6n6S8X"
   },
   "source": [
    "#### Here comes your discussion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cAqf-Pql7xSS",
   "metadata": {
    "id": "cAqf-Pql7xSS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
